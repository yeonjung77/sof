import os
import streamlit as st
from dotenv import load_dotenv
from collections import defaultdict

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from search_timeline import (
    search_keyword_timeline,
    summarize_yearly_insights,
    generate_timeline_synthesis,
)

# ========================================
# ê¸°ë³¸ ì„¤ì •
# ========================================
load_dotenv()
groq_key = os.getenv("GROQ_API_KEY")

if not groq_key:
    st.error("âŒ GROQ_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. .env ë˜ëŠ” Streamlit Secretsì— ë“±ë¡í•´ì£¼ì„¸ìš”.")
    st.stop()


# ========================================
# ë²¡í„°ìŠ¤í† ì–´ ë¡œë”©
# ========================================
@st.cache_resource
def load_vectorstore():
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    return FAISS.load_local(
        "faiss_index", embeddings, allow_dangerous_deserialization=True
    )


# ========================================
# LLM ë¡œë”©
# ========================================
@st.cache_resource
def load_llm():
    return ChatGroq(
        model_name="llama-3.1-8b-instant",
        temperature=0.1,
        groq_api_key=groq_key,
    )


vectorstore = load_vectorstore()
llm = load_llm()
retriever = vectorstore.as_retriever(search_kwargs={"k": 8})

CHAPTER_LABELS = ["Global Economy", "Consumer Shifts", "Fashion System"]


# ========================================
# ë¬¸ì„œ ê·¸ë£¹ ë¡œë”©
# ========================================
@st.cache_resource
def load_grouped_docs():
    all_docs = list(vectorstore.docstore._dict.values())
    by_year_chapter = defaultdict(list)
    by_chapter = defaultdict(list)

    for d in all_docs:
        year = d.metadata.get("year")
        chapter = d.metadata.get("chapter")
        by_year_chapter[(year, chapter)].append(d)
        by_chapter[chapter].append(d)

    return by_year_chapter, by_chapter


by_year_chapter, by_chapter = load_grouped_docs()


# ========================================
# í—¬í¼: ë¬¸ì„œ í¬ë§·íŒ…
# ========================================
def format_docs(docs):
    processed = []
    for d in docs:
        src = os.path.basename(d.metadata.get("source", ""))
        page = d.metadata.get("page", "?")
        year = d.metadata.get("year", "")
        chapter = d.metadata.get("chapter", "")
        header = f"[{year} / {chapter} / {src} p.{page}]"
        processed.append(header + "\n" + d.page_content)
    return "\n\n".join(processed)


# ========================================
# ê³µí†µ RAG í”„ë¡¬í”„íŠ¸
# ========================================
qa_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a professional Fashion MD Research Assistant.\n"
            "Use ONLY the content from McKinsey & BoF 'State of Fashion' (2021â€“2025).\n"
            "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, í•µì‹¬ ìš©ì–´ëŠ” ì˜ì–´ ë³‘ê¸°í•´ì¤˜.",
        ),
        (
            "human",
            "ì§ˆë¬¸: {question}\n\n"
            "ì°¸ê³  ë¬¸ì„œ:\n{context}",
        ),
    ]
)

qa_chain = qa_prompt | llm | StrOutputParser()


# ========================================
# Streamlit UI ì‹œì‘
# ========================================
st.set_page_config(page_title="State of Fashion â€” AI Insight Engine")

st.title("The State of Fashion")
st.title("- AI Insight Engine")
st.caption("AI-powered Insight from SoF 2021â€“2025 Reports")

st.markdown("---")

# ========================================
# ë©”ì¸ íƒ­ êµ¬ì„±
# ========================================
tab_main, tab_keyword, tab_chapter,tab_country = st.tabs([
    "1ï¸âƒ£ AI Report Search",
    "2ï¸âƒ£ Keyword Analytics",
    "3ï¸âƒ£ Chapter Insighs",
    "4ï¸âƒ£ Regional Insights",
])


# ============================================================================
# ğŸ“Œ TAB 1 â€” ì „ì²´ ê²€ìƒ‰ & ì§ˆë¬¸í•˜ê¸°
# ============================================================================
with tab_main:
    st.subheader("Ask Anything â€” AI Analyzes the Report to Answer Your Questions")

    question = st.text_area("ì§ˆë¬¸ ì…ë ¥", key="qa_question")
    chapter_filter = st.selectbox(
        "ê²€ìƒ‰í•  ì±•í„° (ì˜µì…˜)", ["ì „ì²´"] + CHAPTER_LABELS, index=0
    )

    if st.button("AIì—ê²Œ ì§ˆë¬¸í•˜ê¸°", key="qa_button"):
        if not question.strip():
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                docs = vectorstore.similarity_search(question, k=25)

                if chapter_filter != "ì „ì²´":
                    docs = [
                        d for d in docs if d.metadata.get("chapter") == chapter_filter
                    ]
                    docs = docs[:8] or docs

                context = format_docs(docs[:8])
                answer = qa_chain.invoke({"question": question, "context": context})

            st.markdown("### ğŸ“Œ ë‹µë³€")
            st.write(answer)


# ============================================================================
# ğŸ“Œ TAB 2 â€” Chapter Insight (ì„œë¸Œíƒ­ 4ê°œ)
# ============================================================================
with tab_chapter:

    sub1, sub2, sub3 = st.tabs(
        [
            "Annual Keyword Insights",
            "Chapter Keyword Timeline",
            "Keyword Mapping"
        ]
    )

    # ---------------------------------------------------
    # ğŸ“Œ ì„œë¸Œíƒ­ 1 â€” ì—°ë„ë³„ í•µì‹¬ í‚¤ì›Œë“œ
    # ---------------------------------------------------
    with sub1:
        st.subheader("Key Keywords by Year")

        col1, col2 = st.columns(2)
        with col1:
            year = st.selectbox("ì—°ë„ ì„ íƒ", [2021, 2022, 2023, 2024, 2025])
        with col2:
            chapter = st.selectbox("ì±•í„° ì„ íƒ", CHAPTER_LABELS)

        if st.button("í‚¤ì›Œë“œ ìƒì„±", key="year_chapter_summary_keywords"):
            key = (year, chapter)
            docs = by_year_chapter.get(key, [])

            if not docs:
                st.warning("í•´ë‹¹ ì—°ë„/ì±•í„°ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                text = "\n\n".join(d.page_content for d in docs[:20])

                summary_prompt = ChatPromptTemplate.from_messages(
                    [
                        (
                            "system",
                            "You are a senior fashion strategy analyst. "
                            "ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ì—°ë„/ì±•í„°ì˜ í•µì‹¬ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ 5ê°œ ë½‘ì•„ "
                            "ê° í‚¤ì›Œë“œë‹¹ 1~2ë¬¸ì¥ ì„¤ëª…ì„ ë§Œë“¤ì–´ì¤˜.\n"
                            "ì„¤ëª…ì€ í•œêµ­ì–´ë¡œ, ì¤‘ìš”í•œ ìš©ì–´ëŠ” ì˜ì–´ ë³‘ê¸°í•´ì¤˜."
                        ),
                        (
                            "human",
                            "ì—°ë„: {year}\nì±•í„°: {chapter}\n\n"
                            "ë¶„ì„ í…ìŠ¤íŠ¸:\n{text}\n\n"
                            "â¡ ì¶œë ¥ í˜•ì‹:\n"
                            "Key Insights\n"
                            "- í‚¤ì›Œë“œ 1: ì„¤ëª…(1~2ì¤„)\n"
                            "- í‚¤ì›Œë“œ 2: ì„¤ëª…\n"
                            "- í‚¤ì›Œë“œ 3: ì„¤ëª…\n"
                            "- í‚¤ì›Œë“œ 4: ì„¤ëª…\n"
                            "- í‚¤ì›Œë“œ 5: ì„¤ëª…"
                        ),
                    ]
                )

                chain = summary_prompt | llm | StrOutputParser()

                with st.spinner("í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                    summary = chain.invoke(
                        {"year": year, "chapter": chapter, "text": text}
                    )

                st.write(summary)


    # ---------------------------------------------------
    # ğŸ“Œ ì„œë¸Œíƒ­ 2 â€” ì±•í„°ë³„ í‚¤ì›Œë“œ íƒ€ì„ë¼ì¸
    # ---------------------------------------------------
    with sub2:
        st.subheader("Chapter-Based Keyword Timeline Analysis")

        keyword = st.text_input(
            "ë¶„ì„í•  í‚¤ì›Œë“œ (ì˜ˆ: AI, resale, sustainability, Gen Z, silver spenders...)", key="timeline_keyword"
        )
        chapter_sel = st.selectbox(
            "ì±•í„° ì„ íƒ", ["ì „ì²´"] + CHAPTER_LABELS, index=0, key="timeline_chapter"
        )

        if st.button("íƒ€ì„ë¼ì¸ ìƒì„±", key="timeline_button"):
            if not keyword.strip():
                st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                ch = None if chapter_sel == "ì „ì²´" else chapter_sel

                with st.spinner("íƒ€ì„ë¼ì¸ ë¶„ì„ ì¤‘..."):
                    grouped = search_keyword_timeline(keyword, retriever, chapter=ch)

                    timeline_full = {yr: grouped.get(yr, []) for yr in [2021, 2022, 2023, 2024, 2025]}

                    yearly_summary = {}
                    for yr, docs in timeline_full.items():

                        if not docs:
                            yearly_summary[yr] = "âš ï¸ í•´ë‹¹ ì—°ë„ì—ì„œëŠ” í‚¤ì›Œë“œ ì–¸ê¸‰ì´ ê±°ì˜ ì—†ì—ˆìŠµë‹ˆë‹¤."
                        else:
                            text = "\n\n".join(docs[:3])
                            prompt = ChatPromptTemplate.from_messages(
                                [
                                    (
                                        "system",
                                        "You are a fashion trend analyst. "
                                        "ì•„ë˜ í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ í•´ë‹¹ ì—°ë„ì˜ ê´€ì ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.\n"
                                        "â— ì ˆëŒ€ ê¸ˆì§€:\n"
                                        "- '2023ë…„ì˜ í‚¤ì›Œë“œëŠ” ~ì…ë‹ˆë‹¤' ê°™ì€ ë¬¸ì¥ ìƒì„±\n"
                                        "- í…ìŠ¤íŠ¸ì— ì—†ëŠ” ëŒ€í‘œ í‚¤ì›Œë“œ ìƒì„±\n"
                                        "- íŒ¨ì…˜ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì„ ì–¸\n"
                                        "- í•´ì„ ì§€ì–´ë‚´ê¸°\n"
                                        "â— ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ:\n"
                                        "- í…ìŠ¤íŠ¸ ê¸°ë°˜ ìš”ì•½ë§Œ ìƒì„±\n"
                                        "- í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ë˜ í•µì‹¬ ìš©ì–´ë§Œ ì˜ì–´ ë³‘ê¸°"
                                    ),
                                    (
                                        "human",
                                        "í‚¤ì›Œë“œ: {keyword}\nì—°ë„: {year}\n\ní…ìŠ¤íŠ¸:\n{text}"
                                    ),
                                ]
                            )
                            chain = prompt | llm | StrOutputParser()
                            summary = chain.invoke({"keyword": keyword, "year": yr, "text": text})
                            yearly_summary[yr] = summary

                    synthesis_prompt = ChatPromptTemplate.from_messages(
                        [
                            (
                                "system",
                                "You are a senior fashion strategist."
                                "ì—°ë„ë³„ ë¶„ì„ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ íë¦„ì„ ë”± 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.\n"
                                "â— ì ˆëŒ€ ê¸ˆì§€:\n"
                                "- 'ì „ì²´ í‚¤ì›Œë“œëŠ” ~ì…ë‹ˆë‹¤' ë¬¸ì¥ ìƒì„±\n"
                                "- ëŒ€í‘œ í‚¤ì›Œë“œ ì„ ì–¸\n"
                                "- í…ìŠ¤íŠ¸ì— ì—†ëŠ” ê°œë… ì¶”ê°€\n"
                                "â— ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ:\n"
                                "- ìì—°ìŠ¤ëŸ¬ìš´ 3ë¬¸ì¥ ìš”ì•½ë§Œ ìƒì„±"
                            ),
                            (
                                "human",
                                "í‚¤ì›Œë“œ: {keyword}\n\nì—°ë„ë³„ ë‚´ìš©:\n{summary}"
                            ),
                        ]
                    )

                    combined = "\n".join(f"[{yr}] {txt}" for yr, txt in yearly_summary.items())
                    chain = synthesis_prompt | llm | StrOutputParser()
                    synthesis = chain.invoke({"keyword": keyword, "summary": combined})

                st.subheader(f"í‚¤ì›Œë“œ íƒ€ì„ë¼ì¸ : **{keyword}**")

                for yr in [2021, 2022, 2023, 2024, 2025]:
                    st.write(f"### ğŸ“Œ {yr}ë…„")
                    st.write(yearly_summary[yr])
                    st.markdown("---")

                st.write("### ì „ì²´ íë¦„ ìš”ì•½")
                st.write(synthesis)


    # ---------------------------------------------------
    # ğŸ“Œ ì„œë¸Œíƒ­ 3 â€” í‚¤ì›Œë“œ Ã— ì±•í„° ë§¤í•‘
    # ---------------------------------------------------
    with sub3:
        st.subheader("Keyword Mapping Table")

        keyword_map = st.text_input(
            "í‚¤ì›Œë“œ ì…ë ¥ (ì˜ˆ: AI, resale, sustainability, Gen Z, silver spenders...)", key="mapping_keyword"
        )

        if st.button("ë§¤í•‘ ìƒì„±", key="mapping_button"):
            if not keyword_map.strip():
                st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                import pandas as pd

                rows = []

                with st.spinner("ë§¤í•‘ í…Œì´ë¸” ìƒì„± ì¤‘..."):
                    for ch in CHAPTER_LABELS:
                        grouped = search_keyword_timeline(keyword_map, retriever, chapter=ch)

                        # ğŸ“Œ ì±•í„° ë‚´ ê²€ìƒ‰ê²°ê³¼ ì—†ì„ ê²½ìš°
                        if not grouped:
                            rows.append({"Chapter": ch, "Perspective": "ê´€ë ¨ëœ ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."})
                            continue

                        # ì—°ë„ë³„ ìš”ì•½
                        yearly = summarize_yearly_insights(grouped, keyword_map, chapter=ch)

                        # ì—°ë„ë³„ í…ìŠ¤íŠ¸ ì¡°í•©
                        combined = "\n\n".join(
                            f"[{y}]\n{txt}" for y, txt in sorted(yearly.items())
                        )

                        # ğŸ“Œ í•µì‹¬ ë¬¸ì¥ 3ë¬¸ì¥ë§Œ ìƒì„±í•˜ë„ë¡ ì œí•œí•˜ëŠ” í”„ë¡¬í”„íŠ¸
                        map_prompt = ChatPromptTemplate.from_messages(
                            [
                                (
                                    "system",
                                    "You are a fashion strategy analyst."
                                    "ì•„ë˜ ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ì±•í„°ê°€ ì´ í‚¤ì›Œë“œë¥¼ ì–´ë–»ê²Œ ë‹¤ë£¨ëŠ”ì§€ í•µì‹¬ 3ë¬¸ì¥ìœ¼ë¡œë§Œ ì •ë¦¬í•´ì¤˜\n"
                                    "âš ï¸ ì ˆëŒ€ ê¸ˆì§€:\n"
                                    "- 'í‚¤ì›Œë“œ: ~' í˜•ì‹ ë¬¸ì¥ ìƒì„± ê¸ˆì§€\n"
                                    "- '202Xë…„ ~ íë¦„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤' ê¸ˆì§€\n"
                                    "- í…ìŠ¤íŠ¸ì— ì—†ëŠ” ìˆ«ì/ì‚¬ì‹¤/í‚¤ì›Œë“œ ìƒì„± ê¸ˆì§€\n"
                                    "âš ï¸ ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ:\n"
                                    "- í…ìŠ¤íŠ¸ ê¸°ë°˜ í•µì‹¬ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ¬ìš´ 3ë¬¸ì¥ìœ¼ë¡œë§Œ ìš”ì•½\n"
                                    "- í•œêµ­ì–´ë¡œ ì„œìˆ , í•„ìš”í•œ ê²½ìš° í•µì‹¬ ìš©ì–´ë§Œ ì˜ì–´ ë³‘ê¸°"
                                ),
                                (
                                    "human",
                                    "í‚¤ì›Œë“œ: {keyword}\nì±•í„°: {chapter}\n\n"
                                    "ìš”ì•½ í…ìŠ¤íŠ¸:\n{summary}"
                                ),
                            ]
                        )

                        chain = map_prompt | llm | StrOutputParser()

                        perspective = chain.invoke(
                            {
                                "keyword": keyword_map,
                                "chapter": ch,
                                "summary": combined,
                            }
                        )

                        rows.append({"Chapter": ch, "Perspective": perspective})

                df = pd.DataFrame(rows)
                st.table(df)

# =====================================================================
# ğŸ“Œ TAB 2 â€” êµ­ê°€ë³„ ì¸ì‚¬ì´íŠ¸
# =====================================================================
with tab_country:

    st.subheader("ğŸŒ Regional Market Insights (2024 & 2025)")

    country = st.selectbox(
        "êµ­ê°€ ì„ íƒ",
        ["ğŸ‡¯ğŸ‡µ Japan", "ğŸ‡®ğŸ‡³ India", "ğŸ‡ºğŸ‡¸ US", "ğŸ‡¨ğŸ‡³ China", "ğŸ‡ªğŸ‡º EU"],
        index=0,
    )

    # êµ­ê°€ëª…ì„ AIê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    country_map = {
        "ğŸ‡¯ğŸ‡µ Japan": "Japan",
        "ğŸ‡®ğŸ‡³ India": "India",
        "ğŸ‡ºğŸ‡¸ US": "United States",
        "ğŸ‡¨ğŸ‡³ China": "China",
        "ğŸ‡ªğŸ‡º EU": "European Union",
    }
    country_text = country_map[country]

    if st.button("êµ­ê°€ë³„ ì¸ì‚¬ì´íŠ¸ ìƒì„±", key="country_insight"):
        with st.spinner("êµ­ê°€ë³„ ì‹œì¥ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘..."):

            # 1) RAG ê²€ìƒ‰: êµ­ê°€ ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§
            query = f"{country_text} market consumer trend economy fashion"

            docs = vectorstore.similarity_search(query, k=25)

            # ì—°ë„ë³„ ë¶„ë¦¬
            docs_2025 = [d.page_content for d in docs if d.metadata.get("year") == 2025]
            docs_2024 = [d.page_content for d in docs if d.metadata.get("year") == 2024]

            def get_summary(texts, year):
                """LLMì„ ì´ìš©í•œ ì—°ë„ë³„ ìš”ì•½ í•¨ìˆ˜"""
                if not texts:
                    return f"âš ï¸ {year}ë…„ì—ëŠ” í•´ë‹¹ êµ­ê°€ ê´€ë ¨ ì •ë³´ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤."

                combined = "\n\n".join(texts[:8])  # ë„ˆë¬´ ê¸´ ê²½ìš° ì••ì¶•

                prompt = ChatPromptTemplate.from_messages(
                    [
                        (
                            "system",
                            "You are a senior global fashion strategist.\n"
                            "ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ êµ­ê°€ì˜ ì‹œì¥ íŠ¹ì„±ì„ ì •í™•í•˜ê²Œ 3ë¬¸ì¥ìœ¼ë¡œë§Œ ìš”ì•½í•˜ë¼.\n\n"
                            "âš ï¸ ì ˆëŒ€ ê¸ˆì§€:\n"
                            "- 'í•´ë‹¹ êµ­ê°€ì˜ ì‹œì¥ íŠ¹ì„±ì€ ë‹¤ìŒê³¼ ê°™ë‹¤' ê°™ì€ ì„œë¡  ë¬¸ì¥ ìƒì„± ê¸ˆì§€\n"
                            "- í‚¤ì›Œë“œ ì„ ì–¸(ì˜ˆ: '2025ë…„ì˜ í‚¤ì›Œë“œëŠ” ~ì´ë‹¤') ê¸ˆì§€\n\n"
                            "- 'í‚¤ì›Œë“œ: ~' í˜•ì‹ ê¸ˆì§€\n"
                            "- '202Xë…„ì˜ íŠ¸ë Œë“œëŠ” ~ì…ë‹ˆë‹¤' ê¸ˆì§€\n"
                            "- '~ì˜ ì‹œì¥ íŠ¹ì„±ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.' ê¸ˆì§€\n"
                            "- '~ì˜ ì‹œì¥ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.' ê¸ˆì§€\n"
                            "- ì™¸ë˜ ë¬¸ìÂ·ë¹„ìì—°ìŠ¤ëŸ¬ìš´ ì–´êµ¬ ìƒì„± ê¸ˆì§€\n"
                            "- í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì¶”ë¡ /ê°€ì •/ìˆ«ì ìƒì„± ê¸ˆì§€\n"
                            "- ì„œë¡ Â·ê²°ë¡ Â·ì¥ì‹ì  ë¬¸ì¥ ê¸ˆì§€\n\n"
                            "- ê²°ë¡ Â·ì¡°ì–¸ ë¬¸ì¥ ê¸ˆì§€\n"
                            "âš ï¸ ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ:\n"
                            "- í…ìŠ¤íŠ¸ ê¸°ë°˜ í•µì‹¬ë§Œ 3ë¬¸ì¥\n"
                            "- í•œêµ­ì–´ë¡œ ìƒì„±, í•„ìš” ì‹œ í•µì‹¬ ìš©ì–´ë§Œ ì˜ì–´ ë³‘ê¸°"
                            "- ì˜¤ì§ í…ìŠ¤íŠ¸ì— ìˆëŠ” ì‚¬ì‹¤ë§Œ 3ê°œì˜ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬\n"
                            "- ì „ë¬¸ì ì¸ ë¬¸ì²´ ìœ ì§€, ë‹¨ë¬¸/êµ°ë”ë”ê¸° ì—†ëŠ” í‘œí˜„\n"
                            "- í•„ìš”í•œ ê²½ìš°ì—ë§Œ í•µì‹¬ ìš©ì–´ ì˜ì–´ ë³‘ê¸°"
                        ),
                        (
                            "human",
                            f"{year}ë…„ì˜ '{country_text}' ê´€ë ¨ í…ìŠ¤íŠ¸:\n\n{combined}"
                        ),
                    ]
                )

                chain = prompt | llm | StrOutputParser()
                return chain.invoke({})

            summary_2025 = get_summary(docs_2025, 2025)
            summary_2024 = get_summary(docs_2024, 2024)

        # ì¶œë ¥ UI
        st.markdown(f"### ğŸŒ {country_text} â€” Market Insights")

        st.write("### ğŸ“Œ 2025ë…„")
        st.write(summary_2025)
        st.markdown("---")

        st.write("### ğŸ“Œ 2024ë…„")
        st.write(summary_2024)

# =====================================================================
# ğŸ“Œ TAB â€” í‚¤ì›Œë“œ ì‹œê°í™” (Top 10 Bar + Top3 Line Chart)
# =====================================================================
with tab_keyword:

    st.subheader("Top 10 Keywords")

    import re
    from collections import Counter
    import pandas as pd
    import plotly.express as px

    # ---------------------------
    # (A) ê°•í™”ëœ í‚¤ì›Œë“œ í•„í„°ë§ í•¨ìˆ˜
    # ---------------------------
    def extract_keywords(text):
        tokens = re.findall(r"[A-Za-z][A-Za-z\-]+", text)
        tokens = [t.lower() for t in tokens if len(t) > 3]

        stopwords = {
            # ì¼ë°˜ ì˜ì–´ ë¶ˆìš©ì–´
            "that","with","this","have","from","will","into","been","more","than",
            "their","which","also","about","what","when","were","your","them","they",
            "over","only","some","make","made","like","just","very","those","while",
            "where","such","many","each","most","much","other","would","should",
            "could","might","these","both","through","across","there","after","before",
            "under","between","because","based","during","within","without","using",
            "over","well","however","even","though","still","every","including",

            # ìˆ«ì í‘œí˜„
            "percent","million","billion","thousand",

            # íŒ¨ì…˜ ë¬¸ì„œì—ì„œ ë„ˆë¬´ ê¸°ë³¸ì ì¸ ë‹¨ì–´ë“¤
            "brands","brand","business","market","industry","consumer","consumers","customer",
            "customers","global","fashion","system","trend","analysis","report",
            "state","chapter","growth","people","products","product","value",
            "goods","retail","sales","year","years","company","companies",

            # ë¶ˆí•„ìš” í† í°
            "said","https","http","mckinsey",
        }

        tokens = [t for t in tokens if t not in stopwords]

        # ì¶”ê°€ í•„í„°ë§
        tokens = [t for t in tokens if not t.endswith("ing")]     # ë™ëª…ì‚¬ ì œê±°
        tokens = [t for t in tokens if len(set(t)) > 2]           # ë°˜ë³µ ë¬¸ì ì œê±°

        return tokens

    # ---------------------------
    # (B) ì—°ë„ë³„ í…ìŠ¤íŠ¸ ì·¨í•©
    # ---------------------------
    year_texts = {year: "" for year in [2021, 2022, 2023, 2024, 2025]}
    all_docs = list(vectorstore.docstore._dict.values())

    for d in all_docs:
        y = d.metadata.get("year")
        if y in year_texts:
            year_texts[y] += " " + d.page_content

    yearly_keyword_counts = {
        year: Counter(extract_keywords(text))
        for year, text in year_texts.items()
    }

    # ---------------------------
    # (C) ì—°ë„ ì„ íƒ UI
    # ---------------------------
    selected_year = st.selectbox(
        "ì—°ë„ ì„ íƒ",
        [2021, 2022, 2023, 2024, 2025],
        key="keyword_visual_year"
    )

    st.markdown("---")

    # ---------------------------
    # (D) Bar Chart ì¶œë ¥
    # ---------------------------

    top_keywords = yearly_keyword_counts[selected_year].most_common(10)

    if not top_keywords:
        st.warning("í•´ë‹¹ ì—°ë„ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    df_bar = pd.DataFrame({
        "keyword": [k for k, _ in top_keywords],
        "count": [v for _, v in top_keywords],
    })

    fig = px.bar(
        df_bar,
        x="keyword",
        y="count",
        title=f"{selected_year} Keyword Top 10",
        color="count",
        color_continuous_scale="Blues"
    )

    st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")
    st.write("Top 3 Keywords â€” Yearly Trend (2021â€“2025)")

    # ---------------------------
    # (E) ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ ì„ íƒ
    # ---------------------------
    top3_keywords = [k for k, _ in top_keywords[:3]]

    # ---------------------------
    # (F) Top3 í‚¤ì›Œë“œë¥¼ ì—°ë„ë³„ë¡œ ë¹ˆë„ ê¸°ë°˜ ë³€í™” ê³„ì‚°
    # ---------------------------
    for keyword in top3_keywords:
        trend_counts = []
        for yr in [2021, 2022, 2023, 2024, 2025]:
            cnt = yearly_keyword_counts[yr][keyword]
            trend_counts.append(cnt)

        df_line = pd.DataFrame({
            "year": ["2021", "2022", "2023", "2024", "2025"],
            "count": trend_counts
        })

        df_line["year"] = df_line["year"].astype(str)

        st.write(f"ğŸ” {keyword}")

        fig_line = px.line(
            df_line,
            x="year",
            y="count",
            markers=True
        )

        fig_line.update_xaxes(type="category")
        st.plotly_chart(fig_line, use_container_width=True)
        st.markdown("---")
